{"version":3,"sources":["../../src/parser/tokenizeStream.js"],"names":["TokenParsers","name","TokenParser","Tokens","getDataFromStream","stream","data","on","chunk","toString","Promise","resolve","reject","parseNextToken","context","tokenParser","parser","test","originalData","lastLineCharacterCount","lineCount","substring","length","Error","search","parse","tokenizeStream","list","token","push","error"],"mappings":";;;;;;;;;;;AAAA;;AACA;;AAEA;;AAEA;;;;;;AAEA,MAAMA,YAAY,GAAG,gBAAKC,IAAD,IAAU;AACjC;AACA,QAAMC,WAAW,GAAGC,MAAM,CAACF,IAAD,CAA1B;AACAC,EAAAA,WAAW,CAACD,IAAZ,GAAmBA,IAAnB;AACA,SAAOC,WAAP;AACD,CALoB,EAKlB,iBAAKC,MAAL,CALkB,CAArB;;AAOA,MAAMC,iBAAiB,GAAG,MAAOC,MAAP,IAAkB;AAC1C,MAAIC,IAAI,GAAG,EAAX;AACAD,EAAAA,MAAM,CAACE,EAAP,CAAU,MAAV,EAAmBC,KAAD,IAAW;AAC3BF,IAAAA,IAAI,GAAGA,IAAI,IAAIE,KAAK,CAACC,QAAN,EAAf;AACD,GAFD;AAIA,SAAO,IAAIC,OAAJ,CAAY,CAACC,OAAD,EAAUC,MAAV,KAAqB;AACtCP,IAAAA,MAAM,CAACE,EAAP,CAAU,KAAV,EAAiB,MAAMI,OAAO,CAACL,IAAD,CAA9B;AACAD,IAAAA,MAAM,CAACE,EAAP,CAAU,OAAV,EAAmBK,MAAnB;AACD,GAHM,CAAP;AAID,CAVD;;AAYA,MAAMC,cAAc,GAAG,CAACC,OAAD,EAAUR,IAAV,KAAmB;AACxC,QAAMS,WAAW,GAAG,iBAAMC,MAAD,IAAYA,MAAM,CAACC,IAAP,CAAYH,OAAZ,EAAqBR,IAArB,CAAjB,EAA6CN,YAA7C,CAApB;;AACA,MAAI,CAACe,WAAL,EAAkB;AAChB,UAAM;AAAEG,MAAAA;AAAF,QAAmBJ,OAAzB;AACA,UAAM;AAAEK,MAAAA,sBAAF;AAA0BC,MAAAA;AAA1B,QAAwC,oCAC5CF,YAAY,CAACG,SAAb,CAAuB,CAAvB,EAA0BH,YAAY,CAACI,MAAb,GAAsBhB,IAAI,CAACgB,MAArD,CAD4C,CAA9C;AAGA,UAAM,IAAIC,KAAJ,CACH,6BAA4BjB,IAAI,CAACe,SAAL,CAC3B,CAD2B,EAE3Bf,IAAI,CAACkB,MAAL,CAAY,IAAZ,CAF2B,CAG3B,QAAOJ,SAAU,IAAGD,sBAAuB,EAJzC,CAAN;AAMD;;AACD,SAAOJ,WAAW,CAACU,KAAZ,CAAkBX,OAAlB,EAA2BR,IAA3B,CAAP;AACD,CAfD;;AAiBA,MAAMoB,cAAc,GAAG,OAAOZ,OAAP,EAAgB;AAAET,EAAAA;AAAF,CAAhB,KAA+B;AACpD,MAAIC,IAAI,GAAG,MAAMF,iBAAiB,CAACC,MAAD,CAAlC;AACA,MAAIsB,IAAI,GAAG,qBAAK,EAAL,CAAX;AAEAb,EAAAA,OAAO,GAAG,kBAAM,cAAN,EAAsBR,IAAtB,EAA4BQ,OAA5B,CAAV;;AAEA,SAAOR,IAAI,CAACgB,MAAL,GAAc,CAArB,EAAwB;AACtB,QAAI;AACF,YAAMM,KAAK,GAAGf,cAAc,CAACC,OAAD,EAAUR,IAAV,CAA5B;AACAqB,MAAAA,IAAI,GAAGA,IAAI,CAACE,IAAL,CAAUD,KAAV,CAAP;AACAtB,MAAAA,IAAI,GAAGA,IAAI,CAACe,SAAL,CAAeO,KAAK,CAACN,MAArB,CAAP;AACD,KAJD,CAIE,OAAOQ,KAAP,EAAc;AACd;AACA,YAAMA,KAAN;AACD;AACF,GAfmD,CAgBpD;AACA;AACA;AACA;AACA;AACA;;;AACA,SAAOH,IAAP;AACD,CAvBD;;eAyBeD,c","sourcesContent":["import { List } from 'immutable'\nimport { assoc, find, keys, map } from 'ramda'\n\nimport { countLinesAndCharacters } from '../utils'\n\nimport * as Tokens from './tokens'\n\nconst TokenParsers = map((name) => {\n  // eslint-disable-next-line import/namespace\n  const TokenParser = Tokens[name]\n  TokenParser.name = name\n  return TokenParser\n}, keys(Tokens))\n\nconst getDataFromStream = async (stream) => {\n  let data = ''\n  stream.on('data', (chunk) => {\n    data = data += chunk.toString()\n  })\n\n  return new Promise((resolve, reject) => {\n    stream.on('end', () => resolve(data))\n    stream.on('error', reject)\n  })\n}\n\nconst parseNextToken = (context, data) => {\n  const tokenParser = find((parser) => parser.test(context, data), TokenParsers)\n  if (!tokenParser) {\n    const { originalData } = context\n    const { lastLineCharacterCount, lineCount } = countLinesAndCharacters(\n      originalData.substring(0, originalData.length - data.length)\n    )\n    throw new Error(\n      `Do not know how to parse '${data.substring(\n        0,\n        data.search(/\\n/)\n      )}' at ${lineCount}:${lastLineCharacterCount}`\n    )\n  }\n  return tokenParser.parse(context, data)\n}\n\nconst tokenizeStream = async (context, { stream }) => {\n  let data = await getDataFromStream(stream)\n  let list = List([])\n\n  context = assoc('originalData', data, context)\n\n  while (data.length > 0) {\n    try {\n      const token = parseNextToken(context, data)\n      list = list.push(token)\n      data = data.substring(token.length)\n    } catch (error) {\n      // console.log('error:', error)\n      throw error\n    }\n  }\n  // let log = '['\n  // list.forEach((listToken) => {\n  //   log += `  ${listToken.type} length: ${listToken.length},\\n`\n  // })\n  // log += ']'\n  // console.log(log)\n  return list\n}\n\nexport default tokenizeStream\n"],"file":"tokenizeStream.js"}